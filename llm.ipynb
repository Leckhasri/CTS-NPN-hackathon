{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01631d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from groq import Groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21e22790",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "client = Groq(api_key=\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27900f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain_claim_with_llm(\n",
    "    claim_id, provider_id, fraud_prob, predicted_label,\n",
    "    tp, fp, fn, avg_claim_value,\n",
    "    personnel_cost, infra_cost, compliance_cost,\n",
    "    reason_from_model=None\n",
    "):\n",
    "    explanation_prompt = f\"\"\"\n",
    "    You are an AI fraud auditor. Explain why this claim is predicted as {predicted_label}.\n",
    "\n",
    "    Claim ID: {claim_id}\n",
    "    Provider ID: {provider_id}\n",
    "    Fraud Probability: {fraud_prob:.2f}\n",
    "    Prediction: {predicted_label}\n",
    "\n",
    "    Reason from model: {reason_from_model if reason_from_model else \"Provide a general reason based on fraud probability and claim context.\"}\n",
    "\n",
    "    Instructions:\n",
    "    - Give a short, simple explanation in plain language.\n",
    "    - If prediction is FRAUD → explain why it seems suspicious.\n",
    "    - If prediction is NON-FRAUD → explain why it seems normal.\n",
    "    \"\"\"\n",
    "    table_prompt = f\"\"\"\n",
    "    Prepare a neat table with all metrics and costs.\n",
    "\n",
    "    Claim ID: {claim_id}\n",
    "    Provider ID: {provider_id}\n",
    "\n",
    "    Metrics:\n",
    "    - True Positives: {tp}\n",
    "    - False Positives: {fp}\n",
    "    - False Negatives: {fn}\n",
    "    - Avg Claim Value: ${avg_claim_value:,.2f}\n",
    "\n",
    "    Operational Costs (estimated):\n",
    "    - Personnel: ${personnel_cost}\n",
    "    - Infrastructure: ${infra_cost}\n",
    "    - Compliance: ${compliance_cost}\n",
    "    \"\"\"\n",
    "\n",
    "    explanation_response = client.chat.completions.create(\n",
    "        model=\"llama-3.3-70b-versatile\",\n",
    "        messages=[{\"role\": \"user\", \"content\": explanation_prompt}],\n",
    "        temperature=0\n",
    "    )\n",
    "    explanation = explanation_response.choices[0].message.content.strip()\n",
    "    table_response = client.chat.completions.create(\n",
    "        model=\"llama-3.3-70b-versatile\",\n",
    "        messages=[{\"role\": \"user\", \"content\": table_prompt}],\n",
    "        temperature=0\n",
    "    )\n",
    "    table_output = table_response.choices[0].message.content.strip()\n",
    "\n",
    "    return explanation, table_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3fa030b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "claim_id = \"CLM123\"\n",
    "provider_id = \"PRV456\"\n",
    "fraud_prob = 0.87\n",
    "predicted_label = \"FRAUD\"\n",
    "tp, fp, fn = 95, 22, 15\n",
    "avg_claim_value = 1800.0\n",
    "personnel_cost = 5000\n",
    "infra_cost = 1500\n",
    "compliance_cost = 800\n",
    "reason_from_model = \"The claim is unusually expensive compared to similar treatments in the same region.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16f210b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Explanation ===\n",
      "This claim (CLM123) is predicted as FRAUD because it appears to be much more expensive than similar treatments in the same area. This unusual cost raises suspicions, suggesting that the claim might be inflated or fake.\n",
      "\n",
      "=== Metrics & Costs Table ===\n",
      "Here is the neat table with all metrics and costs:\n",
      "\n",
      "| **Category** | **Metric/Cost** | **Value** |\n",
      "| --- | --- | --- |\n",
      "| **Metrics** | True Positives | 95 |\n",
      "|  | False Positives | 22 |\n",
      "|  | False Negatives | 15 |\n",
      "|  | Avg Claim Value | $1,800.00 |\n",
      "| **Operational Costs** | Personnel | $5,000 |\n",
      "|  | Infrastructure | $1,500 |\n",
      "|  | Compliance | $800 |\n",
      "|  | **Total Operational Costs** | **$7,300** |\n",
      "\n",
      "Note: I added a \"Total Operational Costs\" row to calculate the sum of all operational costs. Let me know if you'd like me to add anything else! \n",
      "\n",
      "Also, for reference, here are the claim details:\n",
      "- Claim ID: CLM123\n",
      "- Provider ID: PRV456\n"
     ]
    }
   ],
   "source": [
    "explanation, table_output = explain_claim_with_llm(\n",
    "    claim_id, provider_id, fraud_prob, predicted_label,\n",
    "    tp, fp, fn, avg_claim_value,\n",
    "    personnel_cost, infra_cost, compliance_cost,\n",
    "    reason_from_model\n",
    ")\n",
    "\n",
    "print(\"=== Explanation ===\")\n",
    "print(explanation)\n",
    "print(\"\\n=== Metrics & Costs Table ===\")\n",
    "print(table_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06554044",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pattern",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
