{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27900f2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Metrics saved as:\n",
      "- D:\\CTS Claims project\\metrics_table.csv\n",
      "- D:\\CTS Claims project\\metrics_table.json\n",
      "\n",
      "=== HUMAN-FRIENDLY EXPLANATION ===\n",
      "This claim was classified as \"not fraudulent\" because several factors suggested it was a normal claim. The claim amount was relatively low (under $1000) and the patient's length of stay was short (5 days or less), which are common characteristics of legitimate claims. Additionally, the procedure and diagnosis codes used were consistent with standard practices, further supporting the claim's legitimacy. Overall, the combination of these factors led to a low fraud score, indicating that the claim is likely genuine.\n",
      "\n",
      "=== METRICS JSON (from LLM) ===\n",
      "[\n",
      "    {\n",
      "        \"Claim ID\": \"CLM0000000001\",\n",
      "        \"Provider ID\": \"PRV001253\",\n",
      "        \"Fraud Score\": 0.1915,\n",
      "        \"Prediction\": \"NON-FRAUD\",\n",
      "        \"TP\": 55,\n",
      "        \"FP\": 30,\n",
      "        \"FN\": 25,\n",
      "        \"TN\": 40,\n",
      "        \"Avg Claim Value\": -0.46298859948090226,\n",
      "        \"Personnel Cost\": 5000,\n",
      "        \"Infra Cost\": 1500,\n",
      "        \"Compliance Cost\": 800\n",
      "    }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "from groq import Groq\n",
    "\n",
    "with open(r\"D:\\CTS Claims project\\fraud_explanation.json\", \"r\") as f:\n",
    "    explainable_output = json.load(f)\n",
    "\n",
    "claim_id = explainable_output.get(\"claim_id\", \"N/A\")\n",
    "provider_id = explainable_output.get(\"provider_id\", \"N/A\")\n",
    "fraud_score = explainable_output[\"fraud_score\"]\n",
    "predicted_label = explainable_output[\"predicted_label\"]\n",
    "feature_contributions = explainable_output.get(\"feature_contributions\", {})\n",
    "metrics = explainable_output.get(\"metrics\", {})\n",
    "\n",
    "tp = metrics.get(\"tp\", 0)\n",
    "fp = metrics.get(\"fp\", 0)\n",
    "fn = metrics.get(\"fn\", 0)\n",
    "tn = metrics.get(\"tn\", 0)\n",
    "avg_claim_value = metrics.get(\"avg_claim_value\", 0)\n",
    "personnel_cost = metrics.get(\"personnel_cost\", 0)\n",
    "infra_cost = metrics.get(\"infra_cost\", 0)\n",
    "compliance_cost = metrics.get(\"compliance_cost\", 0)\n",
    "\n",
    "client = Groq(api_key=\"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\") \n",
    "def explain_claim_with_llm():\n",
    "    contrib_text = \"\\n\".join([f\"- {feat}: {weight:.4f}\" for feat, weight in feature_contributions.items()]) \\\n",
    "        if feature_contributions else \"No feature-level explanation available.\"\n",
    "\n",
    "    explanation_prompt = f\"\"\"\n",
    "    You are an expert fraud investigator. Explain this prediction in plain English for a business user.\n",
    "\n",
    "    Claim Details:\n",
    "    - Claim ID: {claim_id}\n",
    "    - Provider ID: {provider_id}\n",
    "    - Fraud Score: {fraud_score:.4f}\n",
    "    - Prediction: {predicted_label}\n",
    "\n",
    "    Why:\n",
    "    {contrib_text}\n",
    "\n",
    "    Metrics:\n",
    "    - TP: {tp}, FP: {fp}, FN: {fn}, TN: {tn}\n",
    "    - Avg Claim Value: ${avg_claim_value:,.2f}\n",
    "    - Costs: Personnel ${personnel_cost}, Infra ${infra_cost}, Compliance ${compliance_cost}\n",
    "\n",
    "    Task:\n",
    "    Explain in 3–4 sentences why this claim was classified this way, using very simple language.\n",
    "    \"\"\"\n",
    "    table_prompt = f\"\"\"\n",
    "    Return ONLY a JSON array (no text, no markdown) with the following fields:\n",
    "    - Claim ID\n",
    "    - Provider ID\n",
    "    - Fraud Score\n",
    "    - Prediction\n",
    "    - TP, FP,FN, TN these values should be not in the confusion matrix values, it should be a real time value (eg: 55 )\n",
    "    - Avg Claim Value\n",
    "    - Personnel Cost\n",
    "    - Infra Cost\n",
    "    - Compliance Cost\n",
    "\n",
    "    Use this data:\n",
    "    Claim ID: {claim_id}\n",
    "    Provider ID: {provider_id}\n",
    "    Fraud Score: {fraud_score:.4f}\n",
    "    Prediction: {predicted_label}\n",
    "    TP: {tp}, FP: {fp}, FN: {fn}, TN: {tn}\n",
    "    Avg Claim Value: {avg_claim_value}\n",
    "    Personnel Cost: {personnel_cost}\n",
    "    Infra Cost: {infra_cost}\n",
    "    Compliance Cost: {compliance_cost}\n",
    "    \"\"\"\n",
    "    explanation_response = client.chat.completions.create(\n",
    "        model=\"llama-3.3-70b-versatile\",\n",
    "        messages=[{\"role\": \"user\", \"content\": explanation_prompt}],\n",
    "        temperature=0\n",
    "    )\n",
    "    explanation = explanation_response.choices[0].message.content.strip()\n",
    "    table_response = client.chat.completions.create(\n",
    "        model=\"llama-3.3-70b-versatile\",\n",
    "        messages=[{\"role\": \"user\", \"content\": table_prompt}],\n",
    "        temperature=0\n",
    "    )\n",
    "    table_json = table_response.choices[0].message.content.strip()\n",
    "\n",
    "    cleaned_json = re.sub(r\"```json|```\", \"\", table_json).strip()\n",
    "    json_match = re.search(r\"(\\[.*\\]|\\{.*\\})\", cleaned_json, re.DOTALL)\n",
    "    if json_match:\n",
    "        cleaned_json = json_match.group(1)\n",
    "\n",
    "    try:\n",
    "        metrics_data = json.loads(cleaned_json)\n",
    "        if isinstance(metrics_data, dict):\n",
    "            metrics_data = [metrics_data]\n",
    "    except json.JSONDecodeError:\n",
    "        raise ValueError(f\"Failed to parse JSON from LLM. Cleaned text:\\n{cleaned_json}\")\n",
    "    df = pd.DataFrame(metrics_data)\n",
    "    csv_file = r\"D:\\CTS Claims project\\metrics_table.csv\"\n",
    "    json_file = r\"D:\\CTS Claims project\\metrics_table.json\"\n",
    "\n",
    "    df.to_csv(csv_file, index=False)\n",
    "    df.to_json(json_file, orient=\"records\", indent=4)\n",
    "\n",
    "    print(f\"✅ Metrics saved as:\\n- {csv_file}\\n- {json_file}\")\n",
    "\n",
    "    return explanation, metrics_data\n",
    "\n",
    "explanation, metrics_data = explain_claim_with_llm()\n",
    "\n",
    "print(\"\\n=== HUMAN-FRIENDLY EXPLANATION ===\")\n",
    "print(explanation)\n",
    "print(\"\\n=== METRICS JSON (from LLM) ===\")\n",
    "print(json.dumps(metrics_data, indent=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f12ac63",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pattern",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
